{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes y replicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos un contexto de Spark y otro de SQL\n",
    "\n",
    "Nota: Cargo desde el inicio todos los métodos/modulos que se usarán a lo largo del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import pyspark.sql \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType\n",
    "from pyspark.sql.types import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkContext(master=\"local\", appName=\"DF y replicación\")\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para eliminar encabezados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropFirstRow(index,iterator):\n",
    "     return iter(list(iterator)[1:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del primer DataFrame\n",
    "\n",
    "Las tres cosas que debes recordar al crear un Dataframe desde un RDDs son:\n",
    "1. En caso de tener encabezado, eliminarlo\n",
    "2. Seleccionar y hacer explícita la seperación de las columnas. Si es necesario castear valores\n",
    "3. Crear el esquema a usarse con los tipos de datos de Spark\n",
    "\n",
    "Cambia el valor de la ruta para que apunte a la ruta donde tienes los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/spark/Downloads/curso-apache-spark-platzi-master/files/\"\n",
    "\n",
    "deportistaOlimpicoRDD =  spark.textFile(path+\"deportista.csv\").map(lambda line : line.split(\",\"))\n",
    "deportistaOlimpico2RDD = spark.textFile(path+\"deportista2.csv\").map(lambda line : line.split(\",\"))\n",
    "deportistaOlimpicoRDD = deportistaOlimpicoRDD.union(deportistaOlimpico2RDD)\n",
    "\n",
    "deportistaOlimpicoRDD=deportistaOlimpicoRDD.mapPartitionsWithIndex(dropFirstRow)\n",
    "\n",
    "deportistaOlimpicoRDD = deportistaOlimpicoRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "int(l[2]),\n",
    "int(l[3]),\n",
    "int(l[4]),\n",
    "float(l[5]),\n",
    "int(l[6])\n",
    "))\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"deportista_id\",IntegerType(),False)     ,\n",
    "StructField(\"nombre\",StringType(),False)        ,\n",
    "StructField(\"genero\",IntegerType(),False)        ,\n",
    "StructField(\"edad\",IntegerType(),True)      ,\n",
    "StructField(\"altura\",IntegerType(),True)        ,\n",
    "StructField(\"peso\",FloatType(),True)      ,\n",
    "StructField(\"equipo_id\",IntegerType(),True)     \n",
    "])\n",
    "\n",
    "deportistaOlimpicoDF = sqlContext.createDataFrame(deportistaOlimpicoRDD,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de DF desde archivo\n",
    "\n",
    "En el caso de la creación de un DF desde cero, solo debemos de indicar la estructura, nombre del archivo y opcionalmente si posee o no encabezado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportesOlimpicosRDDSchema = StructType(\n",
    "    [StructField(\"deporte_id\",IntegerType(),False),\n",
    "     StructField(\"deporte\",StringType(),False)\n",
    "    ])\n",
    "\n",
    "deportesDF = sqlContext.read.schema(deportesOlimpicosRDDSchema).option(\"header\",\"true\").csv(path+\"deporte.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF\n",
    "\n",
    "Nota: Este apartado en el curso se pone al final.\n",
    "\n",
    "Para ejemplificar la función creada por el usuario, cargamos deportistaError el cual tiene ausencia de valores.\n",
    "\n",
    "Con la UDF solucionamos el error. Esta no es una solución definitiva, solo es demostrativa para explicar como crear una UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoRDD =  spark.textFile(path+\"deportistaError.csv\").map(lambda line : line.split(\",\"))\n",
    "deportistaOlimpicoRDD=deportistaOlimpicoRDD.mapPartitionsWithIndex(dropFirstRow)\n",
    "\n",
    "deportistaOlimpicoRDD = deportistaOlimpicoRDD.map(lambda l : (\n",
    "l[0],\n",
    "l[1],\n",
    "l[2],\n",
    "l[3],\n",
    "l[4],\n",
    "l[5],\n",
    "l[6]\n",
    "))\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"deportista_id\",StringType(),False)     ,\n",
    "StructField(\"nombre\",StringType(),False)        ,\n",
    "StructField(\"genero\",StringType(),False)        ,\n",
    "StructField(\"edad\",StringType(),True)      ,\n",
    "StructField(\"altura\",StringType(),True)        ,\n",
    "StructField(\"peso\",StringType(),True)      ,\n",
    "StructField(\"equipo_id\",StringType(),True)     \n",
    "])\n",
    "\n",
    "deportistaError = sqlContext.createDataFrame(deportistaOlimpicoRDD,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaError.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de UDF\n",
    "\n",
    "Los pasos para crear la udf son:\n",
    "\n",
    "1. Crear la función base\n",
    "2. Registrarla como udf\n",
    "3. Indicar al sqlContext que la usaremos como función nativa en sqlContext (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci(value: str) -> int:\n",
    "    return int(value) if len(value) > 0 else None\n",
    "\n",
    "ci_udf = udf(lambda z : ci(z), IntegerType())\n",
    "\n",
    "sqlContext.udf.register(\"ci_udf\", ci_udf)\n",
    "\n",
    "deportistaError.select(ci_udf(\"altura\").alias(\"altura\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaError.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reto\n",
    "\n",
    "Dar vida a todos los archivos como Dataframes.\n",
    "\n",
    "Se anexa una solución probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paisesRDD = spark.textFile(path+\"paises.csv\").map(lambda line : line.split(\",\"))\n",
    "paisesRDD = paisesRDD.mapPartitionsWithIndex(dropFirstRow)\n",
    "\n",
    "paisesRDD = paisesRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "l[2]\n",
    "))\n",
    "\n",
    "schema = StructType([\n",
    "StructField(\"id\",IntegerType(),False),\n",
    "StructField(\"equipo\",StringType(),False),\n",
    "StructField(\"sigla\",StringType(),False)\n",
    "])\n",
    "\n",
    "paisesDF = sqlContext.createDataFrame(paisesRDD,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventoSchema= StructType([\n",
    "    StructField(\"evento_id\",IntegerType(),False),\n",
    "    StructField(\"nombre\",StringType(),False),\n",
    "    StructField(\"deporte_id\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "deportesOlimpicosDF = sqlContext.read.schema(eventoSchema).option(\"header\",\"true\").csv(path+\"evento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "juegoSchema = StructType([\n",
    "    StructField(\"juego_id\",IntegerType(),False),\n",
    "    StructField(\"anio\",StringType(),False),\n",
    "    StructField(\"temporada\",StringType(),False),\n",
    "    StructField(\"ciudad\",StringType(),False),\n",
    "])\n",
    "juegoDF = sqlContext.read.schema(juegoSchema).option(\"header\",\"true\").csv(path+\"juegos.csv\")\n",
    "\n",
    "resultadoSchema = StructType([\n",
    "    StructField(\"resultado_id\",IntegerType(),False),\n",
    "    StructField(\"medalla\",StringType(),False),\n",
    "    StructField(\"deportista_id\",IntegerType(),False),\n",
    "    StructField(\"juego_id\",IntegerType(),False),\n",
    "    StructField(\"evento_id\",IntegerType(),False),\n",
    "])\n",
    "resultadoDF = sqlContext.read.schema(resultadoSchema).option(\"header\",\"true\").csv(path+\"resultados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportesDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportesOlimpicosDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paisesDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juegoDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadoDF.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión de esquema\n",
    "\n",
    "En ocasiones nosotros no creamos los Dataframes y la estrucutra es desconocida para nosotros. con ayuda del método 'printSchema' podemos conocer el esquema del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones de renombrado y eliminación\n",
    "\n",
    "Para renombrar una columna de un DF, podemos usar el método 'withColumnRenamed' o 'alias'.\n",
    "\n",
    "Para eliminar columnas, podemos usar el método 'drop' o simplemente selecionar las columnas que deseamos y sobreesciribr el DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF = deportistaOlimpicoDF.withColumnRenamed(\"genero\",\"sexo\").drop(\"altura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "deportistaOlimpicoDF = deportistaOlimpicoDF.select(\"deportista_id\",\"nombre\",\n",
    "                            col(\"edad\").alias(\"edadAlJugar\"),\"equipo_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de valores\n",
    "\n",
    "Como con el uso de RDDs, podemos usar el método 'filter' para selecionar subconjuntos.\n",
    "\n",
    "filter permite usar operaciones lógicas y de comparación como <,>,>=,!= , &,| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF = deportistaOlimpicoDF.filter( (deportistaOlimpicoDF.edadAlJugar != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF.sort(\"edadAlJugar\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unión de DF\n",
    "\n",
    "Las operaciones conocidas como Join en SQL, tienen una impementación similar, ya que  el método 'join' recibe tres componentes:\n",
    "\n",
    "| Orden | Argumento | Descripción |\n",
    "|-------|--------|-----|\n",
    "|1|dataFrame|dataFrame con el que queremos realizar el cruce|\n",
    "|2|Cruze|Operación lógica a realizar para poder unir los Dataframes|\n",
    "|3|Tipo|El tipo de join a realizar: \"Left\", \"Right\",etc|\n",
    "\n",
    "No olvides que un join es una operación binaria. Por lo que si deseas unir mas DF, deberás realizar multiples joins\n",
    "\n",
    "Posterior a los joins realizados, debemos de realizar una operación select para indicar que valores queremos. \n",
    "\n",
    "En el caso de campos repetidos, podemos hacer explícito el dataframe de origen y para evitar confusón, utilizar alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deportistaOlimpicoDF.join(resultadoDF, deportistaOlimpicoDF.deportista_id == resultadoDF.deporitsta_id,\"left\") \\\n",
    "    .join(juegoDF,juegoDF.juego_id == resultadoDF.juego_id,\"left\") \\\n",
    "    .join(deportesOlimpicosDF, deportesOlimpicosDF.evento_id == resultadoDF.evento_id,\"left\") \\\n",
    "    .select(deportistaOlimpicoDF.nombre,col(\"edad\").alias(\"Edad el jugar\"),\n",
    "           \"medalla\",col(\"anio\").alias(\"Año de juego\"),\n",
    "           deportesOlimpicosDF.nombre.alias(\"Nombre de disciplina\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma forma que una instrucción SQL posee una jerarquía para poder funcionar y retornar correctamente los valores que deseamos. Los DF estan reguidos por las mismas reglas, es decir la misma jerarquía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadoDF.filter(resultadoDF.medalla != \"NA\") \\\n",
    "    .join(deportistaOlimpicoDF,deportistaOlimpicoDF.deportista_id == resultadoDF.deportista_id,\"left\") \\\n",
    "    .join(paisesDF,paisesDF.id == deportistaOlimpicoDF.equipo_id, \"left\") \\\n",
    "    .select(\"medalla\", \"equipo\",\"sigla\").sort( col(\"sigla\").desc() ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones escalares\n",
    "\n",
    "De la misma forma que SQL posee funciones para poder obtener estadísticas. DF hereda el mismo concepto apoyandose de los métodos 'groupBy', \"agg\" y los ya conocidos de sql \"count\",\"sum\",\"avg\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el ejercicio, buscaremos conocer cuantas medallas ha ganado un pais en cada juego olimpico.\n",
    "\n",
    "Primero realizamos la batería de joins que nos permitan identificar todos los valores que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio = deportistaOlimpicoDF.join(resultadoDF, deportistaOlimpicoDF.deportista_id ==resultadoDF.deportista_id,\"left\") \\\n",
    "        .join(juegoDF, juegoDF.juego_id == resultadoDF.juego_id,\"left\") \\\n",
    "        .join(paisesDF,deportistaOlimpicoDF.equipo_id == paisesDF.id,\"left\") \\\n",
    "        .join(deportesOlimpicosDF, deportesOlimpicosDF.evento_id == resultadoDF.evento_id,\"left\") \\\n",
    "        .join(deportesDF,deportesOlimpicosDF.deporte_id == deportesDF.deporte_id,\"left\") \\\n",
    "        .select(\"sigla\",\n",
    "                \"anio\",\n",
    "                \"medalla\",\n",
    "                deportesOlimpicosDF.nombre.alias(\"Nombre subdisciplina\"),\n",
    "                deportesDF.deporte.alias(\"Nombre disciplina\"),\n",
    "                deportistaOlimpicoDF.nombre,   \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previo, identificamos el uso del método \"like\".\n",
    "\n",
    "El cual es util cuando no sabemos el nombre completo o correcto de una columna deseada. \n",
    "\n",
    "En este ejemplo, apartir de todos los juegos de Ski Aplino Femenino, obtenemos la competencias en las que participó el pais. Recuerda que la columna medalla aun posee valores NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.where( col(\"Nombre subdisciplina\").like(\"Alpine Skiing Wo%\")) \\\n",
    "    .sort(\"anio\") \\\n",
    "    .groupBy(\"Sigla\",\"anio\").count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso nos quedamos solo con medallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio2 = medallistaXAnio.filter(medallistaXAnio.medalla != \"NA\") \\\n",
    "    .sort(\"anio\") \\\n",
    "    .groupBy(\"Sigla\",\"anio\",\"Nombre subdisciplina\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forma recomendada para agrupar\n",
    "\n",
    "El método 'agg' es la forma recomendada para hacer agrupaciones ya que brinda la oportunidad de escalar la cantidad de operaciones escalares a realizar en un mismo DF.\n",
    "\n",
    "Es claro que si solo realizamos una operación de agrupación, el uso de 'agg' es excesivo, esta es la recomendación oficial de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio2.groupBy(\"Sigla\",\"anio\") \\\n",
    "    .agg(sum(\"count\").alias(\"Total de medallas\"), \\\n",
    "         avg(\"count\").alias(\"Medallas promedio\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones escalares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejempo realizado para obtener las medallas ganadas por un pais se migará para poder visualizar como sería integrar SQL a un proceso de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadoDF.filter(resultadoDF.medalla != \"NA\") \\\n",
    "    .join(deportistaOlimpicoDF,deportistaOlimpicoDF.deportista_id == resultadoDF.deportista_id,\"left\") \\\n",
    "    .join(paisesDF,paisesDF.id == deportistaOlimpicoDF.equipo_id, \"left\") \\\n",
    "    .select(\"medalla\", \"equipo\",\"sigla\").sort( col(\"sigla\").desc() ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de DF como SQL, se usa registrando un DF como tabla temportal.\n",
    "\n",
    "En el caso de realizar la conexión a una base de datos, este paso puede llegar a ser omitido. Ya que spark estará configurado para poder hacer las conexiones implicitamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadoDF.registerTempTable(\"resultado\")\n",
    "deportistaOlimpicoDF.registerTempTable(\"deportista\")\n",
    "paisesDF.registerTempTable(\"paises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El alias asignado, será la forma en la cual sqlContext conocerá el DF internamente, ahora podemos hacer operaciones de forma tradicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT * FROM deportista\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "                SELECT medalla, equipo, sigla FROM resultado r\n",
    "                JOIN deportista d\n",
    "                ON r.deportista_id = d.deportista_id\n",
    "                JOIN paises p\n",
    "                ON p.id = d.equipo_id\n",
    "                WHERE medalla <> \"NA\"\n",
    "                ORDER BY sigla DESC\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La persistencia de datos no ocurre por defecto en un DF o RDD de Spark, por lo cual debemos de indicar con el método 'cache', por otro lado, para poder verificar si esta almacenado o no, con el método 'is_cached' verificamos su estatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder verificar el tipo de almacenamiento asignado, debemos de conocer el valor de códigos que nos regresa getStorageLevel\n",
    "\n",
    "Para esto, podemos verificar en la documentación de spark:\n",
    "https://spark.apache.org/docs/2.4.6/api/python/_modules/pyspark/storagelevel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder cambiar el tipo de persistencia debemos de primero retirarla y posterior a eso asignarle la que deseamos.\n",
    "\n",
    "Con el método persist, asignaremos la persistencia que nosotros deseamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.rdd.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos crear nuestros propios esquemas de persistencia según las reglas y restricciones de negocio que tengamos en el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def __init__(self, useDisk, useMemory, useOffHeap, deserialized, replication=1):\n",
    "StorageLevel.MEMORY_AND_DISK_3 = StorageLevel(True,True,False,False,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medallistaXAnio.rdd.unpersist()\n",
    "medallistaXAnio.rdd.persist(StorageLevel.MEMORY_AND_DISK_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
